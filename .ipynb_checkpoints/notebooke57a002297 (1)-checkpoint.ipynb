{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
      "Collecting requests[socks]>=2.11.1 (from tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests[socks]>=2.11.1->tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 502kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests[socks]>=2.11.1->tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 957kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests[socks]>=2.11.1->tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 780kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5 (from requests[socks]>=2.11.1->tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 784kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\" (from requests[socks]>=2.11.1->tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/4b/52123768624ae28d84c97515dd96c9958888e8c2d8f122074e31e2be878c/PySocks-1.7.1-py27-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 907kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: chardet, certifi, urllib3, idna, PySocks, requests, oauthlib, requests-oauthlib, six, tweepy\n",
      "Successfully installed PySocks-1.7.1 certifi-2020.12.5 chardet-3.0.4 idna-2.10 oauthlib-3.1.0 requests-2.25.0 requests-oauthlib-1.3.0 six-1.15.0 tweepy-3.9.0 urllib3-1.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* used NLP and NLTK with wordnet et stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efacd59433c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read our dasasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv('../input/mydata/data.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Cleaning the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization, tokenizing, removing stopwards, punctuations, hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the hashtags, mentions and unwanted characters from the tweet texts\n",
    "def clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "clean_tweets = clean_text(datasets, 'Text')\n",
    "clean_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords, punctuations, lemmatize and tokenize word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove stopwords, punctuations, lemmatize and tokenize word\n",
    "# nlp = spacy.load(\"en_core_web_sm\")   #working with english only, no support for swahili\n",
    "nlp = en_core_web_sm.load()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "            \n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "#     for i in text.split():\n",
    "        if i.lower() not in stop:\n",
    "            word = lemmatizer.lemmatize(i)\n",
    "            final_text.append(word.lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "\n",
    "            \n",
    "datasets.tweets = datasets.Text.apply(furnished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Defining the set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Words runs on several different algorithms which compete to get their results higher in the list. One such algorithm uses ***word embedding*** to convert words into many dimensional vectors which represent their meanings\n",
    "\n",
    "Word embedding is any of a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "Link to obtain Related Words: https://relatedwords.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_related_words = '''game disport play foul player mountaineer hunt fence archery sportive fencing jest SUV Boxer sporting\n",
    "sporty pastime fishing attack big game falconry gaud sportless ring sports sailing cross-country hunting skateboarding\n",
    "caving shikar ski surfing turf lake racing preserve\n",
    "professional box angling discus spectator sport toy dive pothunter boxing foil crew shoot pro course fisherman skiing fun rowing fancy\n",
    "hunter trapshooting skating mock judo sportswear mountaineering lark cycling running riding action sport bowl\n",
    "waggish sportful field multisport yachting titlist blood sport race snowboarding javelin sportsmanship wanton weightlifting mockery racket put fan shooting sail law\n",
    "letter championship coursing sporting fish skylark speed skating amateur laugh sports diving ploy climber climb sports diver season\n",
    "kendo letterman swordsman chase daff hammer ballooning surfboarding run wrestle angler the chase fandom paragliding semiprofessional mimic runner quarry slalom game fish\n",
    "activewear scuba diving trainer sportswoman venery luge score tobogganing kiteboarding autocross harlequin wrestling major league unsportsmanlike bait ball junior\n",
    "sportfishing hack synchronized swimming windsurfing falconer train crown stuff defender SUB sky divingtyro\n",
    "pickerel marlin gymnastics crewman netball rage challenge coasteering aficionado langlauf dandiprat spike fool rulebook cave fox hunt\n",
    "wrestler blinder sport vehicle track team joke tubing extreme freediving game bird horse riding referee\n",
    "snowmobiling kit rallying fall sportsperson DCMS speleology marlock rodeo kickball tussle spelunking slump pot racquetball canoeing block dodgeball turn pro\n",
    "broomball commissioner celebrity fumble the top flight sported split time fencer horse racing sport fish\n",
    "goody fox-hunting artistry all-rounder skeleton pro shop ice-skating duffer trash-talk spot bobsledding gamy pig-sticking retire\n",
    "surfer fight misplay rabbit sporting celebrity harness racing survivalist preseason\n",
    "sack try out fisher archer bouldering Dove devotee convert camogie mixed martial arts sim suit up mimicry throw hustle canyoneering sabre hoax pheasant dummy ringed rang AIS territory double bouncebackability mumm extreme sport rung sportingly\n",
    "jape fratch gamer scrimmage sideline ridicule bantam riflery widow close season base \n",
    "jumping allAmerican record  mountainboarding skijoring penalty gamesome merrymaking '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_related_words = '''diplomatic government law election aristotle diplomatical political science\n",
    "political parties politics ethics sovereign state suave smooth tribe republic dynasty expedient sagacious politician direct democracy political economy international relations\n",
    "state public policy power public administration city polis greek language social status border political system niccolò machiavelli thomas hobbes war nationalism technocracy society reactionary anarchism centrism russian empire ottoman empire absolute monarchy constitutions political geography bland monarchy pluralism legislature executive judiciary democracy comparative politics\n",
    "resource academy negotiation force warfare clans tribes company aristocracy plato political philosophy culture chanakya country confucius sic religion governance normative utopia decision-making\n",
    "chimpanzee confederation frontier federation\n",
    "chiefdom africa oasis sumer arthashastra mesopotamia terrorism river kleptocracy athens moralism nations christianism\n",
    "inveterately contradictive mobocracy equalitarian sermonise christianly mudslinger\n",
    "statesmanly sloganeer technocratic megalomanic westernism passivism russophile stateless society ancient civilizations psychodrama\n",
    "self-perpetuating first civilization balkanized piercer\n",
    "clubby internalised band society reflation nontransparent kafkaesque postindustrial unsustainability overregulated conflict theories\n",
    "atomized uruk period self-regulating predynastic egypt hidebound nonideological wrongness nile river dispassion fragmenting squishy heteronormative depersonalized metabolizes oligarchy persian gulf covenantal autocracy\n",
    "euphrates river microbiota legitimacy shapers\n",
    "tigris river solipsistic originalism seven-man standard-setting groupthink eviscerated\n",
    "146-nation classical antiquity neuropolitics greek city states ancient rome referendum nation-state athenian democracy peace of westphalia emer de vattel\n",
    "behavioralism position post-structuralism austrian empire\n",
    "plutocracy kingdom of france kingdom of hungary theocracy spanish empire dictatorship british empire muslim world death of muhammad\n",
    "revolution issue federalism drunk driving australia polity \n",
    "san marino mass media dutch republic steven weber david woodward michel fouc '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_related_words = '''agriculture infrastructure capitalism trading service sector technology  economical supply \n",
    "                          industrialism efficiency frugality retrenchment downsizing   credit debit value \n",
    "                         economize   save  economically\n",
    "                         economies sluggish rise   rising spending conserve trend \n",
    "                         low-management  decline   industry impact poor  \n",
    "                            profession    surplus   fall\n",
    "                         declining  accelerating interest sectors balance stability productivity increase rates\n",
    "                            pushing expanding stabilize  rate industrial borrowing struggling\n",
    "                           deficit predicted    increasing  data\n",
    "                          economizer analysts investment market-based economy   debt free enterprise\n",
    "                         medium  exchange metric savepoint scarcity capital bank company stockholder fund business  \n",
    "                         asset treasury tourism incomes contraction employment jobs upturn deflation  macroeconomics\n",
    "                         bankruptcies exporters hyperinflation dollar entrepreneurship upswing marketplace commerce devaluation \n",
    "                         quicksave deindustrialization stockmarket reflation downspin dollarization withholder bankroll venture capital\n",
    "                         mutual fund plan economy mortgage lender unemployment rate credit crunch central bank financial institution\n",
    "                         bank rate custom duties mass-production black-market developing-countries developing economic-growth gdp trade barter \n",
    "                         distribution downturn economist'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_related_words = '''sociable, gregarious societal friendly society socialization political  sociality \n",
    "                        interpersonal  ethnic socially party welfare public community socialist societies development\n",
    "                            network humans socialism collective personal corporation social constructivism\n",
    "                        relations volition citizenship brute   attitude rights socio \n",
    "                        socioeconomic ethics civic communal marital  sociale socialized communities     \n",
    "                         policy   unions        \n",
    "                        institutions values     governmental   organizations jamboree \n",
    "                         festivity    fairness  support  care  \n",
    "                         sides   activism     unsocial psychosocial \n",
    "                        socializing psychological distributional  demographic  participation reunion \n",
    "                        partygoer partyism festive power network gala housewarming celebration counterparty   social-war\n",
    "                        particularist interactional ideational asocial'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_related_words  = ''' ethnicity heritage modernity spirituality marxismmaterial culture \n",
    "                           ethos nationality humanism romanticism civilisation traditionalism genetics\n",
    "                        kinship heredity marriage   indigenous  archeology  acculturate  \n",
    "                       ontogenesis viniculture modern clothes     rooted \n",
    "                       cicero societies history roots influence geography historical folk origins \n",
    "                       phenomenon teleology ancient aspects perspective liberalism nowadays community style unique prevalent describes \n",
    "                         today  origin   modernity beliefs  genre barbarian ethnic \n",
    "                       colonization cultural universal organization western-civilization structuralism  culture \n",
    "                       heathen pagan transculturation culture peasant classicist nativism anarchy ungrown philosophic cult  \n",
    "                       consciousness islamist bro-culture evolve cultic diaspora aftergrowth native cultural-relativism  \n",
    "                       mongolian cosmopolitan epistemology lifestyles diversity chauvinism westernization materialism vernacular \n",
    "                       homogeneity otherness holism tusculanae disputationes primitivism superficiality hedonism discourse\n",
    "                       puritanism modernism intellectualism  exclusiveness elitism  colonialism  \n",
    "                       pentecostalism paganism nationwide expansion rural  auxesis kimono \n",
    "                       culturize alethophobia nettlebed japanification  dongyi clannishness insularity hybridity\n",
    "                       westernisation foreignness worldview exclusionism enculturation ethnocentrism  confucianist vulgarization\n",
    "                       shintoism  westernism denominationalism    deracination\n",
    "                        eurocentrism  cosmologies  emotiveness bohemianism territorialism\n",
    "                       philosophical-doctrine ethnic minority social-darwinism  theory cultural evolution belief systemfolk music \n",
    "                       traditional art house karl-marx   theorymedia  \n",
    "                       film-theory art history museum studies cultural artifact'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_related_words = '''disease obesity world health organization medicine nutrition well-being exercise welfare wellness health care public health \n",
    "                     nursing stress safety hygiene research social healthy condition aids epidemiology healthiness wellbeing\n",
    "                     care illness medical dieteducation infectious disease environmental healthcare physical fitness hospitals \n",
    "                     health care provider doctors healthy community design insurance sanitation human body patient mental health\n",
    "                      medicare agriculture health science fitnesshealth policy  weight loss physical therapy psychology pharmacy\n",
    "                     metabolic organism human lifestyle status unhealthy upbeat vaccination sleep condom alcohol smoking water family\n",
    "                     eudaimonia eudaemonia air house prevention genetics public families poor needs treatment communicable disease \n",
    "                     study protection malaria development food priority management healthful mental provide department administration\n",
    "                     programs help assistance funding environment improving emergency need program affected schools private mental illness \n",
    "                     treat diseases preparedness perinatal fertility sickness veterinary sanitary pharmacists behavioral midwives\n",
    "                     gerontology infertility hospitalization midwifery cholesterol childcare pediatrician pediatrics medicaid asthma \n",
    "                     pensions sicknesses push-up physical education body-mass-index eat well gymnastic apparatus tune up good morning \n",
    "                     bathing low blood-pressure heart attack health club ride-bike you feel good eczema urticaria dermatitis sunburn overwork \n",
    "                     manufacturing medical sociology need exercise run'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and removing stop words from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# clean the set of words\n",
    "            \n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.lower() not in stop:\n",
    "            word = lemmatizer.lemmatize(i)\n",
    "            final_text.append(word.lower())\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = furnished(economy_related_words)\n",
    "social = furnished(social_related_words)\n",
    "culture = furnished(culture_related_words)\n",
    "health = furnished(health_related_words)\n",
    "politics = furnished(politics_related_words)\n",
    "sport = furnished(sport_related_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = sport\n",
    "words = string1.split()\n",
    "sport = \" \".join(sorted(set(words), key=words.index))\n",
    "sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = politics\n",
    "words = string1.split()\n",
    "politics = \" \".join(sorted(set(words), key=words.index))\n",
    "politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicates\n",
    "string1 = economy\n",
    "words = string1.split()\n",
    "economy = \" \".join(sorted(set(words), key=words.index))\n",
    "economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = social\n",
    "words = string1.split()\n",
    "social = \" \".join(sorted(set(words), key=words.index))\n",
    "social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = culture\n",
    "words = string1.split()\n",
    "culture = \" \".join(sorted(set(words), key=words.index))\n",
    "culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = health\n",
    "words = string1.split()\n",
    "health = \" \".join(sorted(set(words), key=words.index))\n",
    "health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Vectorizing and Standadization.¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the sets of words, then standardizing them. TFIDF will be used in order to take care of the least frequent words. Standardizing is cause TFIDF favors long sentences and there'll be inconsistencies between the length of the tweets and the length of set of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = TfidfVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialvector = get_vectors(social)\n",
    "economic_vector = get_vectors(economy)\n",
    "culture_vector = get_vectors(culture)\n",
    "health_vector = get_vectors(health)\n",
    "politics_vector = get_vectors(politics)\n",
    "sport_vector = get_vectors(sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizing the tweets\n",
    "tv=TfidfVectorizer()\n",
    "# tweets_bowl = tweets_bowl.tweets.apply(get_vectors)\n",
    "# tweets_bowl.head()\n",
    "tfidf_tweets =tv.fit_transform(datasets.tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity is good for cases where duplication does not matter, \n",
    "cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, \n",
    "it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "# jaccard_score(socialvector, economic_vector)\n",
    "\n",
    "#for similarity of 1 and 2 of column1\n",
    "# jaccard_similarity('dog lion a dog','dog is cat')\n",
    "\n",
    "\n",
    "def get_scores(group,tweets):\n",
    "    scores = []\n",
    "    for tweet in tweets:\n",
    "        s = jaccard_similarity(group, tweet)\n",
    "        scores.append(s)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sport scores\n",
    "sp_scores = get_scores(sport, datasets.tweets.to_list())\n",
    "sp_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics scores\n",
    "p_scores = get_scores(politics, datasets.tweets.to_list())\n",
    "p_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# economic scores\n",
    "e_scores = get_scores(economy, datasets.tweets.to_list())\n",
    "e_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social scores\n",
    "s_scores = get_scores(social, datasets.tweets.to_list())\n",
    "s_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# culture scores\n",
    "c_scores = get_scores(culture, datasets.tweets.to_list())\n",
    "c_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health scores\n",
    "h_scores = get_scores(health, datasets.tweets.to_list())\n",
    "h_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''new df with names, and the jaccard scores for each group'''\n",
    "\n",
    "data  = {'names':datasets.user.to_list(), 'economic_score':e_scores,\n",
    "         'social_score': s_scores, 'culture_score':c_scores, 'health_scores':h_scores,'politics_scores':p_scores,'sport_scores':sp_scores}\n",
    "scores_df = pd.DataFrame(data)\n",
    "scores_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actual assigning of classes to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_clusters(l1, l2, l3, l4,l5,l6):\n",
    "    econ = []\n",
    "    socio = []\n",
    "    cul = []\n",
    "    heal = []\n",
    "    poli = []\n",
    "    spor = []\n",
    "    for i, j, k, l, p, s in zip(l1, l2, l3, l4,l5, l6):\n",
    "        m = max(i, j, k, l, p, s)\n",
    "        if m == i:\n",
    "            econ.append(1)\n",
    "        else:\n",
    "            econ.append(0)\n",
    "        if m == j:\n",
    "            socio.append(1)\n",
    "        else:\n",
    "            socio.append(0)        \n",
    "        if m == k:\n",
    "            cul.append(1)\n",
    "        else:\n",
    "            cul.append(0)  \n",
    "        if m == l:\n",
    "            heal.append(1)\n",
    "        else:\n",
    "            heal.append(0)   \n",
    "        if m == p:\n",
    "            poli.append(1)\n",
    "        else:\n",
    "            poli.append(0) \n",
    "        if m == s:\n",
    "            spor.append(1)\n",
    "        else:\n",
    "            spor.append(0) \n",
    "    return econ, socio, cul, heal, poli, spor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = scores_df.economic_score.to_list()\n",
    "l2 = scores_df.social_score.to_list()\n",
    "l3 = scores_df.culture_score.to_list()\n",
    "l4 = scores_df.health_scores.to_list()\n",
    "l5 = scores_df.politics_scores.to_list()\n",
    "l6 = scores_df.sport_scores.to_list()\n",
    "econ, socio, cul, heal, poli, spor = get_clusters(l1, l2, l3, l4, l5, l6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': scores_df.names.to_list(), 'economic':econ, 'social':socio, 'culture':cul, 'health': heal, 'politics': poli, 'sport': spor}\n",
    "cluster_df = pd.DataFrame(data)\n",
    "cluster_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Due to the close similarity between the economic, social and health tweets, some tweets have multiple categories, \n",
    "so to take care of that the rows with a sum > 1 will have to share the categories. After this, clustering will take care \n",
    "of the fractions'''\n",
    "\n",
    "\n",
    "a =  cluster_df[['economic', 'social', 'culture', 'health','politics', 'sport']].sum(axis = 1) > 1\n",
    "c = cluster_df[['economic', 'social', 'culture', 'health','politics', 'sport']].sum(axis = 1)\n",
    "# b = cluster_df.copy()\n",
    "cluster_df.loc[(a), ['economic','social', 'culture', 'health','politics', 'sport']] = 1/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Clustered Datasets: Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_clusters = cluster_df.groupby(['name']).sum()\n",
    "#pivot_clusters['economic'] = pivot_clusters['economic'].astype(int)\n",
    "#pivot_clusters['social'] = pivot_clusters['social'].astype(int)\n",
    "#pivot_clusters['culture'] = pivot_clusters['culture'].astype(int)\n",
    "#pivot_clusters['health'] = pivot_clusters['health'].astype(int)\n",
    "#pivot_clusters['politics'] = pivot_clusters['politics'].astype(int)\n",
    "#pivot_clusters['sport'] = pivot_clusters['sport'].astype(int)\n",
    "pivot_clusters['total'] = pivot_clusters['health'] + pivot_clusters['culture'] + pivot_clusters['social'] +  pivot_clusters['economic'] + pivot_clusters['politics'] + pivot_clusters['sport']\n",
    "pivot_clusters.loc[\"Total\"] = pivot_clusters.sum()  #add a totals row\n",
    "print(pivot_clusters.shape)\n",
    "pivot_clusters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A pie chart to display the total number of tweets in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 20)) \n",
    "a = pivot_clusters.drop(['total'], axis = 1)\n",
    "plt.pie(a.loc['Total'], labels = a.columns)\n",
    "plt.title('A pie chart showing the volumes of tweets under different categories.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge percentage in health could be as a result of the current pandemic, Covid19, everyone is talking about it thus a huge volume of tweets. .\n",
    "\n",
    "The social tweets follow, this could be related to the set of words defined as words related to social. Most of these terms are general, thus if a tweet that maybe was more economy related could have more social words than economy words and thus classified as social, so this is mainnly a bias in the classification method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users with most tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'total', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(1).index, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.total)\n",
    "plt.title('A bar plot showing top tweeps based on volume of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('total tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'sport', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(1).index, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.sport)\n",
    "plt.title('A bar plot showing top tweeps based on volume of sport tweets')\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('sport tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users with most politics tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'politics', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(2).index, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.politics)\n",
    "plt.title('A bar plot showing top tweeps based on volume of politics tweets')\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('politics tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Users with most economy tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'economic', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(2).index, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.economic)\n",
    "plt.title('A bar plot showing top tweeps based on volume of economy tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('economy tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Users with most social tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'social', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(1).index, inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.social)\n",
    "plt.title('A bar plot showing top tweeps based on volume of social tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('social tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users with most culture tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'culture', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(1).index, inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(x = e.index, y = e.culture)\n",
    "plt.title('A bar plot showing top tweeps based on volume of culture tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('culture tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Users with most health tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'health', ascending  = False)\n",
    "e = d.head(15)\n",
    "e.drop(e.head(1).index, inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.barplot(x = e.index, y = e.health)\n",
    "plt.title('A bar plot showing top tweeps based on volume of health tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('user')\n",
    "plt.ylabel('health tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution test and skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import re\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the totals row.\n",
    "print(pivot_clusters.shape)\n",
    "pivot_clusters.drop(pivot_clusters.tail(1).index,inplace=True)\n",
    "print(pivot_clusters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Politics-Sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [0,1]].values\n",
    "X = pivot_clusters[['politics', 'sport']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "#plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "#plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in politics and sport groups')\n",
    "plt.xlabel('politics tweets')\n",
    "plt.ylabel('sport tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Economic-Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [0,1]].values\n",
    "X = pivot_clusters[['economic', 'social']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "# plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "# plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in economic and social groups')\n",
    "plt.xlabel('economic tweets')\n",
    "plt.ylabel('social tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sociol-Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = pivot_clusters.iloc[:, [2,3]].values\n",
    "X = pivot_clusters[['social', 'culture']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "# plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "# plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "# plt.scatter(X[Y_kmeans==5, 0], X[Y_kmeans==5, 1], s=100, c='pink', label= 'Cluster 6')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in culture and social groups')\n",
    "plt.xlabel('social tweets')\n",
    "plt.ylabel('culture tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Sociol-Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [2,4]].values\n",
    "X = pivot_clusters[['social', 'health']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "# plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "# plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in health and social groups')\n",
    "plt.xlabel('social tweets')\n",
    "plt.ylabel('health tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Economic-health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [2,4]].values\n",
    "X = pivot_clusters[['economic', 'health']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "#plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "# plt.scatter(X[Y_kmeans==5, 0], X[Y_kmeans==5, 1], s=100, c='pink', label= 'Cluster 6')\n",
    "# plt.scatter(X[Y_kmeans==6, 0], X[Y_kmeans==6, 1], s=100, c='yellow', label= 'Cluster 7')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in economic and health groups')\n",
    "plt.xlabel('economic tweets')\n",
    "plt.ylabel('health tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Economic-Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [2,4]].values\n",
    "X = pivot_clusters[['economic', 'culture']].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=100, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=100, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=100, c='green', label= 'Cluster 3')\n",
    "plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=100, c='blue', label= 'Cluster 4')\n",
    "# plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=100, c='magenta', label= 'Cluster 5')\n",
    "# plt.scatter(X[Y_kmeans==5, 0], X[Y_kmeans==5, 1], s=100, c='pink', label= 'Cluster 6')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in economic and culture groups')\n",
    "plt.xlabel('economic tweets')\n",
    "plt.ylabel('culture tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Health-Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pivot_clusters.iloc[:, [2,4]].values\n",
    "X = pivot_clusters[['health', 'culture',]].values\n",
    "\n",
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=70, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=70, c='cyan', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=70, c='green', label= 'Cluster 3')\n",
    "plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=70, c='blue', label= 'Cluster 4')\n",
    "# plt.scatter(X[Y_kmeans==4, 0], X[Y_kmeans==4, 1], s=70, c='magenta', label= 'Cluster 5')\n",
    "# plt.scatter(X[Y_kmeans==5, 0], X[Y_kmeans==5, 1], s=100, c='pink', label= 'Cluster 6')\n",
    "# plt.scatter(X[Y_kmeans==6, 0], X[Y_kmeans==6, 1], s=100, c='yellow', label= 'Cluster 7')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in culture and health groups')\n",
    "plt.xlabel('health tweets')\n",
    "plt.ylabel('culture tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * KMeans with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = pivot_clusters.copy()\n",
    "# seg = seg.drop\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(1,8), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "plt.xlabel('components')\n",
    "plt.ylabel('cummulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "pca.fit(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.transform(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(scores)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,15), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('KMeans with PCA clustering')\n",
    "plt.ylabel('wcss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "kmeans_pca = KMeans(n_clusters = n, init = 'k-means++', random_state = 0)\n",
    "kmeans_pca.fit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.concat([seg.reset_index(drop = True), pd.DataFrame(scores)], axis = 1)\n",
    "c.columns.values[-2:] = ['component1', 'component2']\n",
    "c['segment_kmeans_pca'] = kmeans_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "sns.scatterplot(x = c['component1'], y = c['component2'], hue = c['segment_kmeans_pca'], palette=sns.color_palette('coolwarm', n_colors=6),legend=\"brief\")\n",
    "plt.title('Clusters by PCA')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
